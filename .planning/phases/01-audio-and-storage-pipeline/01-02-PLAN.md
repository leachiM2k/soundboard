---
phase: 01-audio-and-storage-pipeline
plan: "02"
type: execute
wave: 2
depends_on:
  - "01-01"
files_modified:
  - src/audio/recorder.ts
autonomous: true
requirements:
  - REC-01
  - REC-02
  - REC-04

must_haves:
  truths:
    - "getMicrophoneStream() is called lazily on first tap, not on app startup"
    - "getMicrophoneStream() reuses a cached MediaStream across recording sessions (avoids re-prompts)"
    - "startRecording() collects audio chunks and assembles a Blob in onstop"
    - "Recording auto-stops after 30 seconds; onWarning fires at 25 seconds"
    - "MediaRecorder.stop() is only called when recorder.state === 'recording' (never throws InvalidStateError)"
    - "Recording uses the RECORDING_MIME_TYPE detected in format.ts (never hardcoded)"
    - "getUserMedia constraints include echoCancellation, noiseSuppression, autoGainControl"
  artifacts:
    - path: "src/audio/recorder.ts"
      provides: "getMicrophoneStream and startRecording — complete recording session management"
      exports: ["getMicrophoneStream", "startRecording", "RecordingResult"]
      min_lines: 70
  key_links:
    - from: "src/audio/recorder.ts"
      to: "src/audio/format.ts"
      via: "imports RECORDING_MIME_TYPE for MediaRecorder options"
      pattern: "RECORDING_MIME_TYPE"
    - from: "src/audio/recorder.ts"
      to: "navigator.mediaDevices.getUserMedia"
      via: "lazy call on first recording attempt"
      pattern: "getUserMedia"
    - from: "src/audio/recorder.ts"
      to: "MediaRecorder"
      via: "instantiated per recording session with RECORDING_MIME_TYPE"
      pattern: "new MediaRecorder"
---

<objective>
Build the recorder module: microphone acquisition and MediaRecorder session management with all iOS Safari guard conditions.

Purpose: The recorder is the most iOS-sensitive piece of the audio pipeline. Every pitfall documented in the research (wrong MIME type, calling stop() on inactive recorder, not caching the MediaStream, calling getUserMedia too early) must be handled correctly here. Retrofitting is expensive.

Output: `src/audio/recorder.ts` with `getMicrophoneStream()` (lazy, cached) and `startRecording()` (30s auto-stop, 25s warning, iOS-safe state guards).
</objective>

<execution_context>
@/Users/rotmanov/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rotmanov/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-audio-and-storage-pipeline/01-CONTEXT.md
@.planning/phases/01-audio-and-storage-pipeline/01-RESEARCH.md
@.planning/phases/01-audio-and-storage-pipeline/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Recorder module with iOS-safe guard conditions</name>
  <files>
    src/audio/recorder.ts
  </files>
  <action>
Create `src/audio/recorder.ts` implementing microphone acquisition and recording session management.

**Locked decisions to implement:**
- Maximum 30 seconds, auto-stop at limit (save everything recorded so far)
- Warning at 25 seconds (onWarning callback)
- No auto-retry on error — caller handles errors with inline message
- Use RECORDING_MIME_TYPE from format.ts (NEVER hardcode a MIME type)
- Cache the MediaStream across sessions (avoids WebKit re-permission bug #215884)
- Do NOT call getUserMedia on app startup — only when the user taps to record
- audioBitsPerSecond: intentionally NOT set — iOS Safari may ignore it; AAC default is fine for speech

**getUserMedia constraints (locked):**
- echoCancellation: true
- noiseSuppression: true
- autoGainControl: true
- sampleRate: NOT set (iOS Safari locks at device rate)
- channelCount: NOT set (iOS defaults mono for voice)
- video: false

```typescript
import { RECORDING_MIME_TYPE } from './format';

export interface RecordingResult {
  blob: Blob;
  mimeType: string; // The actual MIME type of the assembled blob
}

export interface ActiveRecording {
  /** Call to stop recording early (manual stop by user). Triggers onstop → onComplete. */
  stop: () => void;
}

// ------- Microphone stream management -------

const AUDIO_CONSTRAINTS: MediaTrackConstraints = {
  echoCancellation: true,  // Removes mic-from-speaker feedback
  noiseSuppression: true,  // Reduces background noise for speech
  autoGainControl: true,   // Normalizes voice level variations
  // sampleRate: intentionally not set — iOS Safari locks to device rate (44100/48000 Hz)
  // channelCount: not set — iOS defaults to mono for voice; forcing stereo increases file size
};

// Cached stream: reuse across recording sessions to avoid WebKit bug #215884
// (repeated getUserMedia calls in standalone PWA mode trigger re-permission prompts)
let cachedStream: MediaStream | null = null;

/**
 * Acquire the microphone stream lazily (call on first tap, not on app startup).
 * Reuses the cached stream if still active.
 * Throws on NotAllowedError (permission denied), NotFoundError (no mic), NotReadableError (hardware).
 * Caller is responsible for showing the inline error message (per CONTEXT.md locked decision).
 */
export async function getMicrophoneStream(): Promise<MediaStream> {
  if (cachedStream && cachedStream.active) {
    return cachedStream;
  }
  // cachedStream is inactive (tracks ended) — request a fresh stream
  cachedStream = null;
  const stream = await navigator.mediaDevices.getUserMedia({
    audio: AUDIO_CONSTRAINTS,
    video: false,
  });
  cachedStream = stream;
  return stream;
}

// ------- Recording session -------

const WARNING_MS = 25_000; // 25 seconds — warn user before limit
const MAX_MS = 30_000;     // 30 seconds — auto-stop; save everything recorded

/**
 * Start a MediaRecorder session on the provided stream.
 * Returns an ActiveRecording handle with a stop() method for manual early stop.
 *
 * @param stream     MediaStream from getMicrophoneStream()
 * @param onComplete Called with the assembled RecordingResult when recording finishes
 * @param onWarning  Called at 25s (5s before auto-stop); use to show visual indicator
 */
export function startRecording(
  stream: MediaStream,
  onComplete: (result: RecordingResult) => void,
  onWarning?: () => void,
): ActiveRecording {
  const chunks: BlobPart[] = [];

  // Use RECORDING_MIME_TYPE detected at startup (never hardcode — breaks on iOS or Chrome).
  // audioBitsPerSecond is intentionally not set: iOS Safari may ignore it,
  // and AAC default bitrate produces well under 500 KB for 30s of mono speech.
  const options: MediaRecorderInit = RECORDING_MIME_TYPE
    ? { mimeType: RECORDING_MIME_TYPE }
    : {};

  const recorder = new MediaRecorder(stream, options);

  recorder.ondataavailable = (e: BlobEvent) => {
    if (e.data && e.data.size > 0) {
      chunks.push(e.data);
    }
  };

  recorder.onstop = () => {
    // Assemble final blob from all collected chunks.
    // Use recorder.mimeType as fallback in case RECORDING_MIME_TYPE was empty string.
    const finalMimeType = RECORDING_MIME_TYPE || recorder.mimeType;
    const blob = new Blob(chunks, { type: finalMimeType });
    onComplete({ blob, mimeType: blob.type });
  };

  // Warning timer: fire at 25s to allow caller to show visual indicator
  const warnTimer = onWarning ? setTimeout(onWarning, WARNING_MS) : null;

  // Auto-stop timer: stop and save at 30s.
  // Guard with state check: do NOT call stop() if recorder is already inactive
  // (user may have manually stopped just before the timer fires — Pitfall 6).
  const stopTimer = setTimeout(() => {
    if (recorder.state === 'recording') {
      recorder.stop();
    }
  }, MAX_MS);

  // Start with no timeslice: single ondataavailable chunk at stop time.
  // Chunked recording with short timeslices is unreliable on iOS Safari.
  recorder.start();

  return {
    stop: () => {
      // Cancel timers first to prevent double-stop race
      if (warnTimer !== null) clearTimeout(warnTimer);
      clearTimeout(stopTimer);
      // Guard: only call stop() if still recording (Pitfall 6)
      if (recorder.state === 'recording') {
        recorder.stop();
      }
    },
  };
}
```

Ensure the file compiles with `strict: true`. No `any` types. The only import is from `./format`.
  </action>
  <verify>
Run from /Users/rotmanov/git/private/soundboard:
```bash
npm run build
```
Build must complete with zero TypeScript errors. Confirm:
```bash
ls src/audio/recorder.ts
```
  </verify>
  <done>
`npm run build` exits 0. `src/audio/recorder.ts` exists and exports `getMicrophoneStream`, `startRecording`, `RecordingResult`, `ActiveRecording`. No TypeScript errors. No `any` types. RECORDING_MIME_TYPE is imported from format.ts.
  </done>
</task>

</tasks>

<verification>
1. `npm run build` exits 0
2. `src/audio/recorder.ts` exports: `getMicrophoneStream`, `startRecording`, `RecordingResult`, `ActiveRecording`
3. `RECORDING_MIME_TYPE` is imported from `./format` — no hardcoded MIME string in recorder.ts
4. `MediaRecorder.stop()` is guarded with `recorder.state === 'recording'` in BOTH the manual stop path and the auto-stop timer
5. Warning timer fires at 25_000ms, auto-stop timer at 30_000ms
6. No `audioBitsPerSecond` in MediaRecorderInit options
7. `cachedStream` singleton is used to avoid repeated getUserMedia calls
</verification>

<success_criteria>
- Recorder module compiles cleanly under strict TypeScript
- getMicrophoneStream() is lazy (not called at module load) and caches the stream
- startRecording() uses RECORDING_MIME_TYPE from format.ts
- Both the manual stop() and the auto-stop timer guard with recorder.state check
- 25s warning and 30s auto-stop timers are correctly wired and cancelled on manual stop
- No hardcoded MIME types anywhere in recorder.ts
</success_criteria>

<output>
After completion, create `.planning/phases/01-audio-and-storage-pipeline/01-02-SUMMARY.md` documenting:
- All exported symbols and their signatures
- How cachedStream singleton is managed
- Timer values (WARNING_MS, MAX_MS) as implemented
- Any deviations from the plan
</output>
